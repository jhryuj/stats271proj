import tensorflow as tf
from tqdm import trange
import matplotlib.pyplot as plt
import numpy as np

from src.sofc.estimation.lqg import LQG
from tensorflow.linalg import matrix_transpose as transpose

'''
This uses the EM algorithm to approximate the elbo of the data generated by the 
system of equations

x_{t+1} = Ax_t + B u_t + xi_t
y_t = Hx_t + omega_t
z_{t+1} = Az_t - BL_t z_t + K_t (y_t - Hz_t) + eps_t
u_t = - L_t z_t + vareps_t 

x is the state of the world
y is the noisy measurement
z is the latent estimated state of the world (xhat in other functions)
u is the observer control

The experimenter is interested in infering latent states z_t and estimating the 
parameters of the noise terms and the parameters of filters L_t K_t. 

That is we want to estimate the parameters from the given equations: 
-- Observation: 
p(u_t | z_t, x_t) = N(-L_t z_t, \Omega_vareps)
-- Transition: 
p(z_{t+1} | z_t, x_t) = N(a_{t|t-1} z_t + b_{t|t-1}, \Omega_{z_{t|t-1}})
 
'''

class EM_estimation():
    def __init__(self, debug = False):
        '''
        Initialize EM

        '''

        self.debug = debug
        # self.dtype = dtype


    def __call__(self, trainset, dyn, model,
                 lr = 0.01, Niters=5, Niters_mstep = 5):
        '''

        :param x: list of N x (T, batch_size, state_size)
        :param u: list of N x (T, batch_size, state_size)
        :param dyn: dynamics object
        :param obs: observer object
        :param cparams: control parameters to train
        :param sparams: list of N sensory params to train
        :return:
        '''

        obs = trainset['obs']
        x   = trainset['x']
        u   = trainset['u']
        NTB = tf.reduce_sum([xn.shape[0]*xn.shape[1] for xn in x])

        tbar = trange(Niters, desc='Starting EM...', leave=True)
        elbos = []
        for iter in tbar:
            # e-step. Don't keep track of gradients
            #tbar.set_description('(E-step) running...');tbar.refresh();
            expectations = self.e_step(trainset, dyn, model)

            #tbar.set_description('(M-step) running...'); tbar.refresh();
            for m_miter in range(Niters_mstep):
                optimizer = tf.optimizers.Adam(lr) # set new optimizer

                with tf.GradientTape(persistent=True) as t:
                    elbo, elboslike, elbostrans = self.m_step(trainset, dyn, model, expectations)
                    elbo_perNTB = elbo  / tf.cast(NTB,elbo.dtype)
                    nelbo = (-1 * elbo_perNTB)
                grads = t.gradient(nelbo, model.trainable_variables)
                optimizer.apply_gradients(zip(grads, model.trainable_variables))
                gradnorms = tf.linalg.global_norm(grads).numpy()
                tbar.set_description(
                    '(M-step) iter {0:d}: elbo {1:3.2e}, gradnorms {2:3.2e}'.format(
                        iter,elbo_perNTB.numpy().item(),gradnorms))
                tbar.refresh()

                elbos += [elbo_perNTB.numpy().item()]

        return elbos


    def e_step(self, trainset, dyn, model):
        '''
        :return:
        expectations[]: dictionary with following fields
            mu_t|T: list of n (T, Batch_size, state_size) expectations
            Sigma_tp1|T: list of n (T, Batch_size, state_size) variances
        '''

        obs     = trainset['obs']
        x       = trainset['x']
        u       = trainset['u']
        xhat    = trainset['xhat']
        lqgobj  = LQG(dyn)  # just a lightweight LQG object to use repeatedly

        assert len(x) == len(u)
        assert len(x) == len(obs.sparamslist)

        N = len(x) # total number of conditions

        modelobs = model
        parammats =[]
        K = [] # list of n (T, state_size, state_size) Kalman filters
        L = [] # list of n (T, control_size, state_size) Kalman filters

        # calculate filters
        for n in range(N):
            parammat_n  = modelobs.generateParamMat(dyn, n = n)
            Kn, Ln      = lqgobj.calculateFilters(parammat_n, dyn)

            # save in list
            parammats += [parammat_n]
            K += [Kn]
            L += [Ln]

        # calculate statistics for the distribution of conditionals
        # a_{z_{t|t-1}}, b_{z_{t|t-1}}, Sigma_{z_{t|t-1}}
        a, b, S, _,_,_ = self.calculate_conditionals(L,K,x,u,parammats,dyn)

        mu_tt, mu_ttm1, Sigma_tt, Sigma_ttm1 = \
            self.forward_step(a, b, S, K, L, u, x, parammats, dyn)

        mu_tT, Sigma_tT = \
            self.backward_step(mu_tt, mu_ttm1, Sigma_tt, Sigma_ttm1, a, dyn)

        # plot target perception and marginals
        if self.debug and False:
            check00 = tf.norm(mu_tt[0][:,0, 0,0] - mu_tT[0][:,0, 0,0])
            check01 = tf.norm(Sigma_tT[0][:, 0, 0]- Sigma_tt[0][:, 0, 0])

            # plot mu/Sigma
            plt.figure(figsize=(10,8));
            plt.subplot(2,2,1)
            plt.plot(x[0][:,0,0,0],'g:',label = 'target')
            plt.plot(xhat[0][:, 0, 0, 0], 'k:', label='target_estim')

            plt.plot(mu_ttm1[0][:, 0, 0, 0], 'y',
                     alpha=0.4, linewidth=1, label='mu_{t|t-1}')
            var = Sigma_ttm1[0][:, 0, 0]  # variance of the mean muz for target
            plt.fill_between(tf.range(mu_tt[0].shape[0]),
                             mu_ttm1[0][:, 0, 0, 0] - 3 * tf.math.sqrt(var),
                             mu_ttm1[0][:, 0, 0, 0] + 3 * tf.math.sqrt(var),
                             alpha=0.2, color='y', label='mu_{t|t-1} std')

            plt.plot(mu_tt[0][:,0, 0,0], 'r',
                     alpha=0.4, linewidth=1,label='mu_tt')
            var = Sigma_tt[0][:, 0, 0]  # variance of the mean muz for target
            plt.fill_between(tf.range(mu_tt[0].shape[0]),
                             mu_tt[0][:,0, 0,0] - 3 * tf.math.sqrt(var),
                             mu_tt[0][:,0, 0,0] + 3 * tf.math.sqrt(var),
                             alpha=0.2,color='r', label='mu_tt std')

            plt.plot(mu_tT[0][:,0, 0,0], 'b',
                     linewidth=1,label='mu_tT', alpha=0.4)
            var = Sigma_tT[0][:, 0, 0]  # variance of the mean muz for target
            plt.fill_between(tf.range(mu_tT[0].shape[0]),
                             mu_tT[0][:,0, 0,0] - 3 * tf.math.sqrt(var),
                             mu_tT[0][:,0, 0,0] + 3 * tf.math.sqrt(var),
                             alpha=0.2,color='b', label='mu_tT std')
            plt.title('Target position vs mu')
            plt.legend()

            plt.subplot(2,2,2)
            plt.title('Cursor position vs mu')
            dd = 2; dmu = dd;
            plt.plot(x[0][:,0,dd,0],'g:',label = 'target')
            plt.plot(xhat[0][:, 0, dd, 0], 'k:', label='target_estim')

            plt.plot(mu_ttm1[0][:, 0, dmu, 0], 'y',
                     alpha=0.4, linewidth=1, label='mu_{t|t-1}')
            var = Sigma_ttm1[0][:, dmu, dmu]  # variance of the mean muz for target
            plt.fill_between(tf.range(mu_tt[0].shape[0]),
                             mu_ttm1[0][:, 0, dmu, 0] - 3 * tf.math.sqrt(var),
                             mu_ttm1[0][:, 0, dmu, 0] + 3 * tf.math.sqrt(var),
                             alpha=0.2, color='y', label='mu_{t|t-1} std')

            plt.plot(mu_tt[0][:,0, dmu,0], 'r',
                     alpha=0.4, linewidth=1,label='mu_tt')
            var = Sigma_tt[0][:, dmu, dmu]  # variance of the mean muz for target
            plt.fill_between(tf.range(mu_tt[0].shape[0]),
                             mu_tt[0][:,0, dmu,0] - 3 * tf.math.sqrt(var),
                             mu_tt[0][:,0, dmu,0] + 3 * tf.math.sqrt(var),
                             alpha=0.2,color='r', label='mu_tt std')

            plt.plot(mu_tT[0][:,0, dmu,0], 'b',
                     linewidth=1,label='mu_tT', alpha=0.4)
            var = Sigma_tT[0][:, dmu, dmu]  # variance of the mean muz for target
            plt.fill_between(tf.range(mu_tT[0].shape[0]),
                             mu_tT[0][:,0, dmu,0] - 3 * tf.math.sqrt(var),
                             mu_tT[0][:,0, dmu,0] + 3 * tf.math.sqrt(var),
                             alpha=0.2,color='b', label='mu_tT std')
            plt.legend()

            plt.subplot(2,2,3)
            plt.title('Cursor velocity vs mu')
            dd = 3; dmu = dd;
            plt.plot(x[0][:,0,dd,0],'g:',label = 'target')
            plt.plot(xhat[0][:, 0, dd, 0], 'k:', label='target_estim')

            plt.plot(mu_ttm1[0][:, 0, dmu, 0], 'y',
                     alpha=0.4, linewidth=1, label='mu_{t|t-1}')
            var = Sigma_ttm1[0][:, dmu, dmu]  # variance of the mean muz for target
            plt.fill_between(tf.range(mu_tt[0].shape[0]),
                             mu_ttm1[0][:, 0, dmu, 0] - 3 * tf.math.sqrt(var),
                             mu_ttm1[0][:, 0, dmu, 0] + 3 * tf.math.sqrt(var),
                             alpha=0.2, color='y', label='mu_{t|t-1} std')

            plt.plot(mu_tt[0][:,0, dmu,0], 'r',
                     alpha=0.4, linewidth=1,label='mu_tt')
            var = Sigma_tt[0][:, dmu, dmu]  # variance of the mean muz for target
            plt.fill_between(tf.range(mu_tt[0].shape[0]),
                             mu_tt[0][:,0, dmu,0] - 3 * tf.math.sqrt(var),
                             mu_tt[0][:,0, dmu,0] + 3 * tf.math.sqrt(var),
                             alpha=0.2,color='r', label='mu_tt std')

            plt.plot(mu_tT[0][:,0, dmu,0], 'b',
                     linewidth=1,label='mu_tT', alpha=0.4)
            var = Sigma_tT[0][:, dmu, dmu]  # variance of the mean muz for target
            plt.fill_between(tf.range(mu_tT[0].shape[0]),
                             mu_tT[0][:,0, dmu,0] - 3 * tf.math.sqrt(var),
                             mu_tT[0][:,0, dmu,0] + 3 * tf.math.sqrt(var),
                             alpha=0.2,color='b', label='mu_tT std')
            plt.legend()

            plt.subplot(2,2,4)
            plt.title('Cursor acceleration vs mu')
            dd = 4; dmu = dd;
            plt.plot(x[0][:,0,dd,0],'g:',label = 'target')
            plt.plot(xhat[0][:, 0, dd, 0], 'k:', label='target_estim')

            plt.plot(mu_ttm1[0][:, 0, dmu, 0], 'y',
                     alpha=0.4, linewidth=1, label='mu_{t|t-1}')
            var = Sigma_ttm1[0][:, dmu, dmu]  # variance of the mean muz for target
            plt.fill_between(tf.range(mu_tt[0].shape[0]),
                             mu_ttm1[0][:, 0, dmu, 0] - 3 * tf.math.sqrt(var),
                             mu_ttm1[0][:, 0, dmu, 0] + 3 * tf.math.sqrt(var),
                             alpha=0.2, color='y', label='mu_{t|t-1} std')

            plt.plot(mu_tt[0][:,0, dmu,0], 'r',
                     alpha=0.4, linewidth=1,label='mu_tt')
            var = Sigma_tt[0][:, dmu, dmu]  # variance of the mean muz for target
            plt.fill_between(tf.range(mu_tt[0].shape[0]),
                             mu_tt[0][:,0, dmu,0] - 3 * tf.math.sqrt(var),
                             mu_tt[0][:,0, dmu,0] + 3 * tf.math.sqrt(var),
                             alpha=0.2,color='r', label='mu_tt std')

            plt.plot(mu_tT[0][:,0, dmu,0], 'b',
                     linewidth=1,label='mu_tT', alpha=0.4)
            var = Sigma_tT[0][:, dmu, dmu]  # variance of the mean muz for target
            plt.fill_between(tf.range(mu_tT[0].shape[0]),
                             mu_tT[0][:,0, dmu,0] - 3 * tf.math.sqrt(var),
                             mu_tT[0][:,0, dmu,0] + 3 * tf.math.sqrt(var),
                             alpha=0.2,color='b', label='mu_tT std')
            plt.legend()

            plt.show()

        # plot
        if self.debug and False:
            # plot mu/Sigma
            plt.figure(figsize=(10,8))
            plt.subplot(2,1,1)
            # plt.plot(x[0][:,0,0,0],'g:',label = 'target')
            plt.plot(xhat[0][:, 0, 0, 0], 'k:', label='target_estim')

            plt.plot(mu_tT[0][:,0, 0,0], 'b',
                     linewidth=1,label='mu_tT', alpha=0.4)
            var = Sigma_tT[0][:, 0, 0]  # variance of the mean muz for target
            plt.fill_between(tf.range(mu_tT[0].shape[0]),
                             mu_tT[0][:,0, 0,0] - 3* tf.math.sqrt(var),
                             mu_tT[0][:,0, 0,0] + 3* tf.math.sqrt(var),
                             alpha=0.2,color='b', label='mu_tT 3std')
            plt.title('Target position estimates')
            plt.xlim([0, 100])
            plt.legend()

            plt.subplot(2,1,2)
            plt.title('Target velocity estimates')
            dd = 1; dmu = dd;
            plt.plot(xhat[0][:, 0, dd, 0], 'k:', label='target_estim')

            plt.plot(mu_tT[0][:,0, dmu,0], 'b',
                     linewidth=1,label='mu_tT', alpha=0.4)
            var = Sigma_tT[0][:, dmu, dmu]  # variance of the mean muz for target
            plt.fill_between(tf.range(mu_tT[0].shape[0]),
                             mu_tT[0][:,0, dmu,0] - tf.math.sqrt(var),
                             mu_tT[0][:,0, dmu,0] + tf.math.sqrt(var),
                             alpha=0.2,color='b', label='mu_tT 1std')
            plt.xlim([0,10])
            plt.legend()
            plt.show()

        expectations = {'mu_tT': mu_tT, 'Sigma_tT': Sigma_tT,
                        'mu_tt': mu_tt, 'Sigma_tt': Sigma_tt,
                        'mu_ttm1': mu_ttm1, 'Sigma_ttm1':Sigma_ttm1}

        if self.debug and False:
            logpz, logpzlist = self.prob_latent(expectations, xhat, dyn)
            plt.figure(figsize=(10,8))
            plt.plot(logpzlist[0][:,0,0])
            plt.title('Log p(z|x_1:T)')
            plt.xlabel('Timeframes')
            plt.ylabel('logp')
            plt.show()

        return expectations

    def forward_step(self, a, b, S, K, L, u, x, parammats,dyn):
        '''

        :return:
        mu_t|t
        Sigma_t
        mu_t|tm1
        Sigma_t|tm1 # p(z_t | u_{1:t-1})
        '''

        assert len(a) == len(b) and len(a) == len(S) and len(a) == len(L)
        N = len(a)

        mu_tt        = [] # N x (T,B,SS-us,1)
        mu_ttm1      = [] # N x (T,B,SS-us,1)
        Sigma_tt     = [] # N x (T,B,SS-us,SS-us)
        Sigma_ttm1   = [] # N x (T,B,SS-us,SS-us)

        # stochastic state index
        ss_idx  = dyn.state_stochastic_idx
        idx     = dyn.estimable_idx

        for n in range(N):
            ss = int(tf.reduce_sum(tf.cast(idx,tf.int8)))

            a_n         = a[n] # (T,ss,ss)
            b_n         = b[n] # (T,bs,ss,1)
            S_n         = S[n] # (T,ss,ss)
            K_n         = K[n] # (T,ss,ms)
            L_n         = tf.boolean_mask(L[n],idx,axis=2) # (T,cs,ss)
            L_n_full    = L[n]
            u_n         = u[n] # (T, bs, cs, 1)
            x_n         = x[n]
            params_n    = parammats[n]

            A       = params_n['A']
            B       = params_n['B']
            B_sto   = tf.boolean_mask(B, ss_idx, axis=0)
            H       = params_n['H']
            C0f     = tf.boolean_mask(params_n['C0f'], ss_idx ,axis=0)
            C0f_full = params_n['C0f']
            CA      = params_n['CA']
            SA      = params_n['SA']
            EA      = tf.boolean_mask(params_n['EA'],idx,axis=0)
            Sw      = SA @ transpose(SA) # turn std into variance
            Omega_u = CA @ transpose(CA) # turn std into variance
            Sz      = EA @ transpose(EA)
            Omega_x = C0f @ transpose(C0f)
            Omega_x_full = C0f_full @ transpose(C0f_full)

            T, ss1, _ = a_n.shape # (T,ss,ss)
            T, batch_size, ss2, _  = b_n.shape # (T,batch_size, ss,ss)

            assert ss == ss1 and ss == ss2

            # start at zero mean and zero covariance (likelihood dominates)
            mu_ttm1_n    = []
            Sigma_ttm1_n = []
            mu_tt_n      = []
            Sigma_tt_n   = []

            for t in range(0, T):
                if t == 0:
                    mu_prior    = b_n[t,:,:,:] # likelihood only p(z|x0)
                    Sigma_prior = S_n[t, :, :] # p(z_0 | x_0) likelihood only
                else:
                    mu_prior     = a_n[t,None,:,:] @ mu_tt_n[t-1] + b_n[t,:,:,:]
                    Sigma_prior  = a_n[t,:,:] @ Sigma_tt_n[t-1] @ \
                                      tf.transpose(a_n[t,:,:]) + S_n[t,:,:]

                if self.debug:
                    # check if invertible
                    #aaaa = a_n[t, :, :] @ Sigma_tt_n[-1] @ tf.transpose(a_n[t, :, :])
                    #tf.linalg.cholesky(aaaa)
                    tf.linalg.cholesky(Sigma_prior)

                # innovation and kalman filter
                rtfull = x_n[t+1, :, :, :] - (
                    A[None, :, :] @  x_n[t, :, :, :]
                    - B[None,:,:] @ L_n[t, None, :,:] @ mu_prior) # innovation

                # stochastic part
                rt = tf.boolean_mask(rtfull, ss_idx ,axis= 1)

                condvar = (B_sto @ L_n[t,:,:]
                           @ Sigma_prior
                           @ tf.transpose(B_sto @ L_n[t,:,:])) \
                          + Omega_x + (B_sto @ Omega_u @ transpose(B_sto))

                # estimable_idx = tf.reduce_sum(condvar,axis=0) == 0
                invsqrtcondvar = tf.linalg.inv(tf.linalg.cholesky(condvar))
                Rt  = Sigma_prior @ tf.transpose(-1 * B_sto @ L_n[t,:,:]) @ \
                      tf.transpose(invsqrtcondvar) @ invsqrtcondvar

                # stats after observation
                mu_post    = mu_prior + Rt[None,:,:] @ rt
                Sigma_post = (tf.eye(Sigma_prior.shape[0],dtype=x[0].dtype) -
                              Rt @ (-1 * B_sto @ L_n[t,:,:])) @ Sigma_prior

                if self.debug and False:
                    # todo: the innovation term is dominating the prior term??
                    # check if invertible
                    tf.linalg.cholesky(Sigma_post)
                    check00 = tf.norm(mu_post - mu_prior)
                    check01 = tf.norm(Sigma_post - Sigma_prior)
                    check02 = tf.norm(Rt)
                    check03 = tf.norm(rt)
                    if t == 50:
                        check10 = 1; #Stop here and check a bit
                        Rt[0,0] # weight of the likelihood on the next state position

                    assert not tf.reduce_any(tf.math.is_nan(mu_post)) and \
                           not tf.reduce_any(tf.math.is_nan(mu_prior)) and \
                           not tf.reduce_any(tf.math.is_nan(Sigma_post)) and \
                           not tf.reduce_any(tf.math.is_nan(Sigma_prior))

                mu_tt_n      += [mu_post]
                mu_ttm1_n    += [mu_prior]
                Sigma_tt_n   += [Sigma_post]  # initial variance?
                Sigma_ttm1_n += [Sigma_prior]

            mu_tt       += [tf.stack(mu_tt_n)]
            mu_ttm1     += [tf.stack(mu_ttm1_n)]
            Sigma_tt    += [tf.stack(Sigma_tt_n)]
            Sigma_ttm1  += [tf.stack(Sigma_ttm1_n)]

        return mu_tt, mu_ttm1, Sigma_tt, Sigma_ttm1

    def backward_step(self, mu_tt, mu_ttm1, Sigma_tt, Sigma_ttm1, a, dyn):
        '''

        :return:
        mu_t|T
        Sigma_t|T
        '''

        if self.debug:
            assert len(a) == len(mu_tt) and len(a) == len(mu_ttm1) \
                   and len(a) == len(Sigma_tt) and len(a) == len(Sigma_ttm1)

        N = len(a)

        mu_tT        = []
        Sigma_tT     = []

        for n in range(N):
            idx = dyn.estimable_idx
            ss = int(tf.reduce_sum(tf.cast(idx,tf.int8)))

            mu_tt_n      = mu_tt[n]
            mu_ttm1_n    = mu_ttm1[n]
            Sigma_tt_n   = Sigma_tt[n]
            Sigma_ttm1_n = Sigma_ttm1[n]
            a_n          = a[n]

            T, ss1, _ = a_n.shape
            Tm1,batch_size,ss2,_ = mu_tt_n.shape

            # start at zero mean and zero covariance (likelihood dominates)
            mu_tT_n      = [mu_tt_n[-1,:,:,:]] # when t = T
            Sigma_tT_n   = [Sigma_tt_n[-1,:,:]] # initial variance?

            for t in range(Tm1-2, -1, -1):
                # actual recursive computation here...
                invsqrtSigmatp1 = tf.linalg.inv(tf.linalg.cholesky(Sigma_ttm1_n[t+1,:,:]))
                Jt      = Sigma_tt_n[t,:,:] @ transpose(a_n[t+1,:,:]) @ \
                          transpose(invsqrtSigmatp1)@ invsqrtSigmatp1
                mu      = mu_tt_n[t,:,:,:] + Jt[None,:,:] @ (mu_tT_n[-1] - mu_ttm1_n[t+1,:,:,:])
                Sigma   = Sigma_tt_n[t,:,:] + Jt @ (Sigma_tT_n[-1]  - Sigma_ttm1_n[t+1,:,:]) @ \
                          transpose(Jt)

                if self.debug:
                    check00 = tf.norm(mu - mu_tt_n[t,:,:,:])
                    check01 = tf.norm(Sigma - Sigma_tt_n[t,:,:])

                    check1 = tf.norm(Sigma_tT_n[-1] - Sigma_ttm1_n[t + 1, :, :])
                    check2 = tf.norm(Jt @ (Sigma_tT_n[-1]  - Sigma_ttm1_n[t+1,:,:]) @
                                     transpose(Jt))
                    check3 = tf.norm(Sigma)
                    if t == 50:
                        check10 = 1; #Stop here and check a bit

                    assert not tf.reduce_any(tf.math.is_nan(mu)) and \
                           not tf.reduce_any(tf.math.is_nan(Sigma))

                mu_tT_n      += [mu]
                Sigma_tT_n   += [Sigma]

            mu_tT       += [tf.reverse(tf.stack(mu_tT_n),[0])]
            Sigma_tT    += [tf.reverse(tf.stack(Sigma_tT_n),[0])]

        return mu_tT, Sigma_tT

    def m_step(self, trainset, dyn, model, expectations):
        '''

        :return:
        loglike

        '''

        idx     = dyn.estimable_idx
        ss      = int(tf.reduce_sum(tf.cast(idx, tf.int8)))

        obs     = trainset['obs']
        x       = trainset['x']
        u       = trainset['u']
        # xhat  = trainset['xhat'] = xhat_list
        lqgobj  = LQG(dyn)  # just a lightweight LQG object to use repeatedly

        assert len(x) == len(u)
        assert len(x) == len(obs.sparamslist)

        N = len(x) # total number of conditions

        modelobs = model

        ss_idx = dyn.state_stochastic_idx

        # calculate filters and store parameters again
        parammats =[]; K = []; L = [];
        for n in range(N):
            parammat_n  = modelobs.generateParamMat(dyn, n = n)
            Kn, Ln      = lqgobj.calculateFilters(parammat_n, dyn)
            parammats += [parammat_n]; K += [Kn]; L += [Ln];

        # calculate statistics for the distribution of conditionals
        # a_{z_{t|t-1}}, b_{z_{t|t-1}}, Sigma_{z_{t|t-1}}
        a, b, Omega_zttm1, _,_,_ = self.calculate_conditionals(L,K,x,u,parammats,dyn)

        mu_tT       = expectations['mu_tT']
        Sigma_tT    = expectations['Sigma_tT']

        # compute elbo
        elbo = 0
        elbos_like = []
        elbos_trans = []

        for n in range(N):
            elboslike_n = []
            elbostrans_n = []

            params_n    = parammats[n]
            A   = params_n['A']
            B   = params_n['B']
            H   = params_n['H']
            CA  = params_n['CA']
            SA  = params_n['SA']
            EA = tf.boolean_mask(params_n['EA'], idx, axis=0)
            Omega_w = SA @ tf.transpose(SA) # turn std into variance
            Omega_u = CA @ tf.transpose(CA) # turn std into variance
            invsqrtOmega_u = tf.linalg.inv(tf.linalg.cholesky(Omega_u))
            Omega_z = EA @ transpose(EA)

            C0f = tf.boolean_mask(params_n['C0f'], ss_idx ,axis=0)
            C0f_full = params_n['C0f']
            Omega_x = C0f @ transpose(C0f)
            Omega_x_full = C0f_full @ transpose(C0f_full)

            mu_n     = mu_tT[n]
            Sigma_n  = Sigma_tT[n]

            a_n     = a[n]
            b_n     = b[n]
            Omegaz_n = Omega_zttm1[n]

            u_n     = u[n]
            x_n     = x[n]
            L_n = tf.boolean_mask(L[n], idx, axis=2)  # (T,cs,ss)

            T = a_n.shape[0]

            # prior term for z0
            Sz0         = Omegaz_n[0,:,:]
            invsqrtsz0  = tf.linalg.inv(tf.linalg.cholesky(Sz0))
            elbo += -1/2 * tf.reduce_sum(
                tf.cast(Sz0.shape[-1] * tf.math.log(2*np.pi),Sz0.dtype) +
                tf.linalg.logdet(Sz0) +
                tf.linalg.trace(
                    transpose(invsqrtsz0[None, :, :]) @ invsqrtsz0[None,:,:] @ (
                    Sigma_n[0, None, :,:] + mu_n[0,:,:,:] @ transpose(mu_n[0,:,:,:])
                )))

            assert not tf.math.is_nan(elbo) and not tf.math.is_inf(elbo)

            # transitions
            # todo: t=1 is weird still
            for t in range(1,T):
                invsqrtOmegaz_t = tf.linalg.inv(tf.linalg.cholesky(Omegaz_n[t, None, :, :]))
                res1 = (mu_n[t,:,:,:] - a_n[t,None,:,:] @ mu_n[t-1,:,:,:]
                       - b_n[t,:,:,:])
                quad = res1 @ transpose(res1) + \
                       Sigma_n[t, None, :, :] - \
                       a_n[t,None,:,:] @ Sigma_n[t-1,None,:,:] @ transpose(a_n[t,None,:,:])
                elbotrans =  -1/2 * tf.reduce_sum(
                    tf.cast(Omegaz_n.shape[-1] * tf.math.log(2*np.pi),Omegaz_n.dtype) +
                    tf.linalg.logdet(Omegaz_n[t,None,:,:])
                    + tf.linalg.trace(transpose(invsqrtOmegaz_t) @ invsqrtOmegaz_t @ quad))

                elbo += elbotrans
                elbostrans_n += [elbotrans]

                # big terms here
                if self.debug and t == 50:
                    check30 = tf.reduce_mean(tf.linalg.trace(quad))
                    check31 = tf.reduce_mean(tf.linalg.trace(res1 @ transpose(res1)))
                    check32 = tf.reduce_mean(tf.linalg.trace(Sigma_n[t, None, :, :] ))
                    check33 = tf.reduce_mean(tf.linalg.trace(Omegaz_n[t, None, :, :]))
                    check34 = tf.reduce_sum(tf.linalg.trace(transpose(invsqrtOmegaz_t) @ invsqrtOmegaz_t @ quad))
                    check35 = tf.linalg.logdet(Omegaz_n[t,None,:,:])

                    res2 = (mu_n[t, :, :, :] - mu_n[t - 1, :, :, :])
                    res3 = (mu_n[t, :, :, :] - a_n[t,None,:,:] @ mu_n[t-1,:,:,:])
                    res4 = (mu_n[t, :, :, :] - b_n[t,:,:,:])
                    check312 = tf.reduce_mean(tf.linalg.trace(res2 @ transpose(res2)))
                    check313 = tf.reduce_mean(tf.linalg.trace(res3 @ transpose(res3)))
                    check314 = tf.reduce_mean(tf.linalg.trace(res4 @ transpose(res4)))

                    if not tf.math.is_finite(tf.reduce_max(tf.abs(tf.math.log(Omegaz_n[t, None, :, :])))):
                        oz = tf.clip_by_value(Omegaz_n[t, None, :, :],
                                              clip_value_min=1e-10,clip_value_max=1e10)

                if self.debug and tf.abs(elbotrans) > 1000 and False:
                    # this can be caused by really small Omega_z (small estimation noise)
                    print('elbo explosion. check for numerical stability issues')

                if self.debug and False:
                    # Shumway and Stoeffer 1982 formulation.
                    # Mathematically equivalent but seems to be numerically unstable
                    P_t = Sigma_n[t, None, :, :] + mu_n[t, :, :, :] @ transpose(
                        mu_n[t, :, :, :])
                    P_tm1 = Sigma_n[t - 1, None, :, :] + mu_n[t - 1, :, :,
                                                         :] @ transpose(
                        mu_n[t - 1, :, :, :])
                    P_ttm1 = a_n[t, None, :, :] @ Sigma_n[t - 1, None, :,
                                                  :] + mu_n[t, :, :,
                                                       :] @ transpose(
                        mu_n[t - 1, :, :, :])

                    # transitions
                    invsqrtOmegaz_t = tf.linalg.inv(
                        tf.linalg.cholesky(Omegaz_n[t, None, :, :]))
                    elbotrans = -1 / 2 * tf.linalg.logdet(
                        Omegaz_n[t, None, :, :])
                    elbotrans += -1 / 2 * tf.reduce_sum(tf.linalg.trace(
                        transpose(invsqrtOmegaz_t) @ invsqrtOmegaz_t @ (
                                P_t
                                + a_n[t, None, :, :] @ P_tm1 @ transpose(
                            a_n[t, None, :, :])
                                - a_n[t, None, :, :] @ transpose(P_ttm1)
                                - P_ttm1 @ transpose(a_n[t, None, :, :])
                                - mu_n[t, :, :, :] @ transpose(b_n[t, :, :, :])
                                - b_n[t, :, :, :] @ transpose(mu_n[t, :, :, :])
                                + a_n[t, None, :, :] @ mu_n[t - 1, :, :,
                                                       :] @ transpose(
                            b_n[t, :, :, :])
                                + b_n[t, :, :, :] @ transpose(
                            mu_n[t - 1, :, :, :]) @ transpose(
                            a_n[t, None, :, :])
                                + b_n[t, :, :, :] @ transpose(b_n[t, :, :, :])
                        )))

                    # check equivalence (omegazt seems problematic)
                    check20 = transpose(invsqrtOmegaz_t) @ invsqrtOmegaz_t @ (P_t
                            + a_n[t, None, :, :] @ P_tm1 @ transpose(a_n[t, None, :, :])
                            - a_n[t, None, :, :] @ transpose(P_ttm1)
                            - P_ttm1 @ transpose(a_n[t, None, :, :])
                            - mu_n[t, :, :, :] @ transpose(b_n[t, :, :, :])
                            - b_n[t, :, :, :] @ transpose(mu_n[t, :, :, :])
                            + a_n[t, None, :, :] @ mu_n[t - 1, :, :,:] @ transpose(b_n[t, :, :, :])
                            + b_n[t, :, :, :] @ transpose(mu_n[t - 1, :, :, :]) @ transpose(a_n[t, None, :, :])
                            + b_n[t, :, :, :] @ transpose(b_n[t, :, :, :]))
                    check21 =  transpose(invsqrtOmegaz_t) @ invsqrtOmegaz_t @ (res1 @ transpose(res1)
                                + Sigma_n[t, None, :, :]
                                - a_n[t,None,:,:] @ Sigma_n[t-1,None,:,:] @ transpose(a_n[t,None,:,:]))

                    check22 = tf.norm(check20-check21) # should be none.
                    plt.figure(figsize=(8,10));
                    plt.subplot(2, 1, 1)
                    plt.imshow(check20[0,:,:])
                    plt.colorbar()
                    plt.subplot(2,1,2)
                    plt.imshow(check21[0, :, :])
                    plt.colorbar()
                    plt.show()

                    if t == 50:
                        check20 = 1 # stop here.

                assert not tf.math.is_nan(elbo) and not tf.math.is_inf(elbo)

            # likelihoods
            # todo: t= 1 is when first velocity comes in.
            # this is unestimable, so the velocity residual pops up dramatically
            # i just start from t=2 (and acceleration residual)
            for t in range(0, T):
                # p(x_{t+1} | z_t)
                resx    = (x_n[t+1, :, :, :] -
                           (A @ x_n[t, :, :, :]
                            - B @ L_n[t, None, :, :] @ mu_n[t, :, :, :]))
                quadx   = resx @ transpose(resx) +(
                        B @ L_n[t,None,:,:] @ Sigma_n[t,None,:,:]
                        @ transpose(B @ L_n[t,None,:,:]))
                Omega_xlike     = Omega_x_full + B @ Omega_u @ transpose(B)
                invsqrtOmegax   = tf.linalg.inv(tf.linalg.cholesky(Omega_xlike))
                elbolike   =  -1 / 2 * tf.reduce_sum(
                    tf.cast(Omega_xlike.shape[-1] * tf.math.log(2*np.pi),Omega_xlike.dtype) +
                    tf.linalg.logdet(Omega_xlike) +
                    tf.linalg.trace(transpose(invsqrtOmegax)
                                    @ invsqrtOmegax @ quadx))

                # use p(x_{t+1} | z_t)
                elbo        += elbolike
                elboslike_n += [elbolike]

                assert not tf.math.is_nan(elbo) and not tf.math.is_inf(elbo)

            elbos_like  += [tf.stack(elboslike_n)]
            elbos_trans += [tf.stack(elbostrans_n)]

        return elbo, elbos_like, elbos_trans

    def calculate_conditionals(self,L,K,x,u,parammats,dyn):
        '''
        latent state transition conditional parameters
        '''
        a = []
        b = []
        S = []

        a_full = []
        b_full = []
        S_full = []

        assert len(L) == len(K) and len(L) == len(x) and \
               len(L) == len(x) and len(L) == len(parammats)
        N = len(L)

        # identify indices of unestimable states
        idx = dyn.estimable_idx

        for n in range(N):
            L_n         = L[n]
            K_n         = K[n]
            x_n         = x[n]
            u_n         = u[n]
            params_n    = parammats[n]

            A   = params_n['A']
            B   = params_n['B']
            H   = params_n['H']
            CA  = params_n['CA']
            SA  = params_n['SA']
            EA  = tf.boolean_mask(params_n['EA'],idx,axis=0) # estimation noise
            Sy = SA @ tf.transpose(SA) # turn std into variance
            Sz = EA @ tf.transpose(EA)

            Tp1, batch_size, SS, _ = x_n.shape # (T,B,SS,1)
            SS_estimable = int(tf.reduce_sum(tf.cast(idx, x_n.dtype)))

            an = [tf.zeros((SS_estimable,SS_estimable),dtype=x[0].dtype)]
            bn = [tf.boolean_mask(K_n[0, None,:, :], idx, axis=1) @ H[None,:,:] @ x_n[0,:,:,:]]
            Sn = [tf.boolean_mask(K_n[0, :, :],idx,axis=0) @ Sy @
                  tf.transpose(tf.boolean_mask(K_n[0, :, :],idx,axis=0)) + Sz]

            an_full = [tf.zeros((SS,SS),dtype=x[0].dtype)]
            bn_full = [K_n[0, None,:, :]@ H[None,:,:] @ x_n[0,:,:,:] ]
            Sn_full = [K_n[0, :, :] @ Sy @ transpose(K_n[0, :, :]) +
                       params_n['EA'] @ transpose(params_n['EA'])]

            for t in range(1,Tp1-1):
                at = tf.boolean_mask(tf.boolean_mask(A,idx,axis=0),idx,axis=1) - \
                     tf.boolean_mask(B,idx,axis=0) @ tf.boolean_mask(L_n[t-1,:,:],idx,axis=1) - \
                     tf.boolean_mask(K_n[t,:,:],idx,axis=0) @ tf.boolean_mask(H,idx,axis=1) # (ss-us,ss-us)
                bt = tf.boolean_mask(K_n[t, None,:, :],idx, axis=1) @ \
                     H[None,:,:] @ x_n[t,:,:,:] # make broadcasting explicit
                St = tf.boolean_mask(K_n[t, :, :],idx,axis=0) @ Sy @ \
                     tf.transpose(tf.boolean_mask(K_n[t, :, :],idx,axis=0)) + Sz

                at_full = A - B @ L_n[t - 1, :, :] - K_n[t, :, :] @ H
                bt_full = K_n[t, None, :, :] @ H[None, :, :] @ x_n[t, :, :,:]  # make broadcasting explicit
                St_full = K_n[t, :, :] @ Sy @ transpose(K_n[t, :, :]) + \
                          params_n['EA'] @ transpose(params_n['EA'])

                an += [at]
                bn += [bt]
                Sn += [St]

                an_full += [at_full]
                bn_full += [bt_full]
                Sn_full += [St_full]

                if self.debug:
                    tf.linalg.cholesky(St) # check if invertible
                    check00 = K_n[t, None, :, :] @ H[None,:,:] @ x_n[t,:,:,:]
                    if t == 50:
                        check00 = at

            # subtract non-estimable states
            a += [tf.stack(an)] # N x (T,SS-US,SS-US), not dependent on batch
            b += [tf.stack(bn)] # N x (T,B,SS-US,SS-US)
            S += [tf.stack(Sn)] # N x (T,SS-US,SS-US), not dependent on batch

            a_full += [tf.stack(an_full)]
            b_full += [tf.stack(bn_full)]
            S_full += [tf.stack(Sn_full)]

        return a, b, S, a_full, b_full, S_full

    def calculate_apprx_elbo(self,trainset, dyn, model):
        # run one step
        obs = trainset['obs']
        x   = trainset['x']
        u   = trainset['u']
        NTB = tf.reduce_sum([xn.shape[0]*xn.shape[1] for xn in x])

        # calculate expectations
        expectations = self.e_step(trainset, dyn, model)

        # calculate elbo
        elbo, elbolike, elbotrans = self.m_step(trainset, dyn, model, expectations)
        elbo         = elbo  / tf.cast(NTB,elbo.dtype)

        if self.debug and False:
            plt.figure()
            plt.subplot(2,1,1)
            for n in range(len(elbolike)):
                plt.plot(elbolike[n])
            plt.title('Likelihood elbo')
            #plt.yscale('log')
            plt.xlabel('Timeframes')

            plt.subplot(2, 1, 2)
            for n in range(len(elbolike)):
                plt.plot(elbotrans[n])
            plt.title('Transition elbo')
            #plt.yscale('log')
            plt.xlabel('Timeframes')

            plt.show()

        elbolike_sum = 0
        elbotrans_sum = 0
        for n in range(len(elbolike)):
            elbolike_sum += tf.reduce_sum(elbolike[n])
            elbotrans_sum += tf.reduce_sum(elbotrans[n])

        results = {}
        results['elbo']         = tf.squeeze(elbo).numpy()
        results['elbolike']     = tf.squeeze((elbolike_sum / tf.cast(NTB,elbo.dtype))).numpy()
        results['elbotrans']    = tf.squeeze((elbotrans_sum / tf.cast(NTB,elbo.dtype))).numpy()
        results['NTB']          = tf.cast(NTB,elbo.dtype)
        results['expectations'] = expectations
        results['elbolike_all']  = elbolike
        results['elbotrans_all'] = elbotrans

        return results

    def plot_target_vs_model(self, lqgobj, dyn, modelobs, x, L, K, mu, sigma):
        raise NotImplementedError

        plot_xt_list, plot_xhat_list, plot_ut_list, plot_tt_list = \
            lqgobj.simulate(dyn, modelobs, x_fix=x, L=L, K=K, batch_size=1)

        plt.figure()
        plt.plot(x[0][:, 0, 0, 0], 'k', label='target')
        plt.plot(x[0][:, 0, 2, 0], 'g', label='obs cursor')
        plt.plot(plot_xt_list[0][:, 0, 2, 0], 'r', label='modelsim cursor')
        plt.plot(plot_xhat_list[0][:, 0, 0, 0], 'y', linewidth=1,
                 label='modelsim_estim')
        var = sigma[0][:, 0, 0]  # variance of the mean muz for target
        plt.plot(mu[0][:, 0, 0, 0], 'b', linewidth=1, label='model mu_t')
        plt.fill_between(tf.range(mu[0].shape[0]),
                         mu[0][:, 0, 0, 0] - 3 * tf.math.sqrt(var),
                         mu[0][:, 0, 0, 0] + 3 * tf.math.sqrt(var),
                         alpha=0.5, color='b', label='model mu_t std')
        plt.title('Observer vs model and mu_tT')
        plt.legend()
        plt.show()

        plt.figure();
        plt.subplot(2, 2, 1)
        plt.plot(x[0][:, 0, 0, 0], 'k', label='target')
        plt.plot(mu[0][:, 0, 0, 0], 'b', linewidth=1, label='model mu_t')
        var = Sigma_tT[0][:, 0, 0]  # variance of the mean muz for target
        plt.fill_between(tf.range(mu[0].shape[0]),
                         mu[0][:, 0, 0, 0] - 3 * tf.math.sqrt(var),
                         mu[0][:, 0, 0, 0] + 3 * tf.math.sqrt(var),
                         alpha=0.5, color='b', label='model mu_t std')
        plt.title('Target position vs mu_tT')

        plt.subplot(2, 2, 2)
        plt.plot(x[0][:, 0, 2, 0], 'k', label='cursor')
        plt.plot(mu_tT[0][:, 0, 1, 0], 'b', linewidth=1, label='model mu_t')
        var = Sigma_tT[0][:, 1, 1]  # variance of the mean muz for target
        plt.fill_between(tf.range(mu_tT[0].shape[0]),
                         mu_tT[0][:, 0, 1, 0] - 3 * tf.math.sqrt(var),
                         mu_tT[0][:, 0, 1, 0] + 3 * tf.math.sqrt(var),
                         alpha=0.5, color='b', label='model mu_t std')
        plt.title('Cursor position vs mu_tT')

        plt.subplot(2, 2, 3)
        plt.plot(x[0][:, 0, 3, 0], 'k', label='cursor vel')
        plt.plot(mu_tT[0][:, 0, 2, 0], 'b', linewidth=1, label='model mu_t')
        var = Sigma_tT[0][:, 2, 2]  # variance of the mean muz for target
        plt.fill_between(tf.range(mu_tT[0].shape[0]),
                         mu_tT[0][:, 0, 2, 0] - 3 * tf.math.sqrt(var),
                         mu_tT[0][:, 0, 2, 0] + 3 * tf.math.sqrt(var),
                         alpha=0.5, color='b', label='model mu_t std')
        plt.title('Cursor velocity vs mu_tT')

        plt.subplot(2, 2, 4)
        plt.plot(x[0][:, 0, 4, 0], 'k', label='cursor')
        plt.plot(mu_tT[0][:, 0, 3, 0], 'b', linewidth=1, label='model mu_t')
        var = Sigma_tT[0][:, 3, 3]  # variance of the mean muz for target
        plt.fill_between(tf.range(mu_tT[0].shape[0]),
                         mu_tT[0][:, 0, 3, 0] - 3 * tf.math.sqrt(var),
                         mu_tT[0][:, 0, 3, 0] + 3 * tf.math.sqrt(var),
                         alpha=0.5, color='b', label='model mu_t std')
        plt.title('Cursor acceleration vs mu_tT')
        plt.show()

    def prob_latent(self, expectations, xhat, dyn):
        '''
        calculates probability of xhat given the expectations
        :param expectations:
        :param xhat:
        :return:
        # log probability of seeing xhat
        '''

        NTB = tf.reduce_sum([xn.shape[0]*xn.shape[1] for xn in xhat])
        idx = dyn.estimable_idx

        N = len(expectations['mu_tT'])

        lp = 0
        lplist = []
        for n in range(N):
            mu      = expectations['mu_tT'][n] # T x B x D x 1
            Sigma   = expectations['Sigma_tT'][n] # T x D x D
            xhat_n  = tf.boolean_mask(xhat[n],idx,axis=2) # T x B x D x 1

            lp_n    = self.MVN_logpdf(xhat_n, mu, Sigma)
            lplist += [lp_n]
            lp      += tf.reduce_sum(lp_n)

        return (lp / tf.cast(NTB,lp.dtype)).numpy(), lplist

    def MVN_logpdf(self, data, mean, sigma):
        # data: T x B x D x 1
        # mean: T x B x D x 1
        # Sigma: T x D x D

        D = data.shape[-2]
        L = tf.linalg.cholesky(sigma)  # K x D x D cholesky decomposition
        T = min(data.shape[0], mean.shape[0])
        xs = tf.linalg.inv(L)[:T,None,:,:] @ (data[:T,:,:,:] - mean[:T,:,:,:])
        lp = -0.5 * tf.reduce_sum(xs ** 2, axis=-2)
        #Ldiag = np.einsum('...ii -> ...i', L)
        #logdet_sqrt = np.sum(np.log(abs(Ldiag)), axis=-1)
        norm = - 0.5 * D * np.log(2 * np.pi) - tf.math.log(tf.linalg.trace(L))
        lp += norm[:, None, None]  # (...,)

        if self.debug and False:
            plt.figure(figsize=(10, 8))
            plt.plot(tf.norm(xs,axis=2)[:,0,0])
            plt.xlabel('timeframe')
            plt.ylabel('logpz')

        return lp  #

    def prob_latent_generative(self, trainset, model, dyn):
        '''
        prob latent from generative model

        :param trainset:
        :param model:
        :param dyn:
        :return:
        '''
        obs     = trainset['obs']
        x       = trainset['x']
        u       = trainset['u']
        xhat    = trainset['xhat']

        NTB     = tf.reduce_sum([xn.shape[0]*xn.shape[1] for xn in xhat])
        N       = len(x)
        lqgobj  = LQG(dyn)  # just a lightweight LQG object to use repeatedly

        lplist = []
        parammats = []; K = []; L = [];
        for n in range(N):
            parammat_n  = model.generateParamMat(dyn, n=n)
            Kn, Ln      = lqgobj.calculateFilters(parammat_n, dyn)
            # save in list
            parammats += [parammat_n]
            K += [Kn]
            L += [Ln]

        # calculate statistics for the distribution of conditionals
        # a_{z_{t|t-1}}, b_{z_{t|t-1}}, Sigma_{z_{t|t-1}}
        a, b, S, _, _, _ = \
            self.calculate_conditionals(L, K, x, u, parammats, dyn)

        lplike  = 0
        lptrans = 0
        lp_like = []    # p(x_{t+1}| z_t, x_t, theta, phi )
        lp_trans = []   # p(z_{t+1}| z_t, x_{t+1}, theta, phi )
        for n in range(N):
            mu          = a[n][1:,None,:,:] @ xhat[n][:-1,:,:,:] + b[n][1:,:,:,:] # (T,SS,SS)
            Sigma       = S[n][1:,:,:]
            lp_trans    += [self.MVN_logpdf(xhat[n][1:,:,:,:], mu, Sigma)]
            lptrans     += tf.reduce_sum(lp_trans[-1])

            if self.debug and True:
                res = xhat[n][51, 0, :, :] - mu[50, 0, :, :]

            params_n    = parammats[n]
            A           = params_n['A']
            B           = params_n['B']
            CA          = params_n['CA']
            C0f         = params_n['C0f']
            Omega_u     = CA @ transpose(CA)  # turn std into variance
            Omega_x     = C0f @ transpose(C0f)

            mu      = A[None,None,:,:] @ x[n][:-1,:,:,:] - \
                      B[None,None,:,:] @ L[n][:,None,:,:] @ xhat[n]  # (T,SS,SS)
            Sigma   = Omega_x + B @ Omega_u @ transpose(B)
            lp_like += [self.MVN_logpdf(x[n][1:,:,:], mu, Sigma[None,:,:])]
            lplike  = tf.reduce_sum(lp_like[-1])

        # plot stuff
        if self.debug and False:
            plt.figure(figsize=(10,8))
            plt.subplot(2,1,1)
            plt.plot(lp_like[0][:,0,0])
            plt.title('Likelihood')
            plt.xlabel('Timeframe')
            plt.ylabel('log prob')

            plt.subplot(2, 1, 2)
            plt.plot(lp_trans[0][:, 0, 0])
            plt.title('Transition')
            plt.xlabel('Timeframe')
            plt.ylabel('log prob')
            plt.show()

        return lp_like, lp_trans,\
               (lplike / tf.cast(NTB, lplike.dtype)).numpy(), \
               (lptrans / tf.cast(NTB, lptrans.dtype)).numpy()